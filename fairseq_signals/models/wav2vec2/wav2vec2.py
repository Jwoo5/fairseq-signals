# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

from dataclasses import dataclass, field
from typing import List, Tuple
from omegaconf import II

import torch
import torch.nn as nn
import numpy as np

from fairseq_signals.dataclass import ChoiceEnum
from fairseq_signals.models import register_model
from fairseq_signals.models.conv_transformer import ConvTransformerConfig, ConvTransformerModel
from fairseq_signals.modules import (
    GradMultiply,
    GumbelVectorQuantizer,
)
from fairseq_signals.utils.utils import buffered_arange

@dataclass
class Wav2Vec2Config(ConvTransformerConfig):
    logit_temp: float = field(
        default=0.1, metadata={"help": "temperature to divide logits by"}
    )
    quantize_targets: bool = field(
        default=False, metadata={"help": "use quantized targets"}
    )
    quantize_input: bool = field(
        default=False, metadata={"help": "use quantized inputs"}
    )
    same_quantizer: bool = field(
        default=False, metadata={"help": "use same quantizer for inputs and targets"}
    )
    target_glu: bool = field(
        default=False, metadata={"help": "adds projection + glu to targets"}
    )
    latent_vars: int = field(
        default=320,
        metadata={"help": "number of latent variables V in each group of the codebook"},
    )
    latent_groups: int = field(
        default=2,
        metadata={"help": "number of groups G of latent variables in the codebook"},
    )
    latent_dim: int = field(
        default=0,
        metadata={
            "help": "if > 0, uses this dimensionality for latent variables. "
            "otherwise uses final_dim / latent_groups"
        },
    )

    # negative selection
    num_negatives: int = field(
        default=100,
        metadata={"help": "number of negative examples from the same sample"},
    )
    negatives_from_everywhere: bool = field(
        default=False,
        metadata={"help": "sample negatives from everywhere, not just masked states"},
    )
    cross_sample_negatives: int = field(
        default=0, metadata={"help": "number of negative examples from the any sample"}
    )
    codebook_negatives: int = field(
        default=0, metadata={"help": "number of negative examples codebook"}
    )

    latent_temp: Tuple[float, float, float] = field(
        default=(2, 0.5, 0.999995),
        metadata={
            "help": "temperature for latent variable sampling. "
            "can be tuple of 3 values (start, end, decay)"
        },
    )

@register_model("wav2vec2", dataclass=Wav2Vec2Config)
class Wav2Vec2Model(ConvTransformerModel):
    def __init__(self, cfg: Wav2Vec2Config):
        super().__init__(cfg)
        self.cfg = cfg

        self.quantizer = None
        self.input_quantizer = None

        self.n_negatives = cfg.num_negatives
        self.cross_sample_negatives = cfg.cross_sample_negatives
        self.codebook_negatives = cfg.codebook_negatives
        self.negatives_from_everywhere = cfg.negatives_from_everywhere

        self.logit_temp = cfg.logit_temp
        self.linear = nn.Linear(768, 256)


        if cfg.quantize_targets:
            vq_dim = cfg.latent_dim if cfg.latent_dim > 0 else self.final_dim
            self.quantizer = GumbelVectorQuantizer(
                dim=self.embed,
                num_vars=cfg.latent_vars,
                temp=cfg.latent_temp,
                groups=cfg.latent_groups,
                combine_groups=False,
                vq_dim=vq_dim,
                time_first=True,
            )
            self.project_q = nn.Linear(vq_dim, self.final_dim)
        else:
            self.project_q = nn.Linear(self.embed, self.final_dim)

        if cfg.quantize_input:
            if cfg.same_quantizer and self.quantizer is not None:
                vq_dim = self.final_dim
                self.input_quantizer = self.quantizer
            else:
                vq_dim = cfg.latent_dim if cfg.latent_dim > 0 else cfg.embed_dim
                self.input_quantizer = GumbelVectorQuantizer(
                    dim=self.embed,
                    num_vars=cfg.latent_vars,
                    temp=cfg.latent_temp,
                    groups=cfg.latent_groups,
                    combine_groups=False,
                    vq_dim=vq_dim,
                    time_first=True,
                )
            self.project_inp = nn.Linear(vq_dim, self.embed_dim)

        self.target_glu = None
        if cfg.target_glu:
            self.target_glu = nn.Sequential(
                nn.Linear(self.final_dim, self.final_dim * 2), nn.GLU()
            )

        self.final_proj = nn.Linear(cfg.encoder_embed_dim, self.final_dim)

    def upgrade_state_dict_named(self, state_dict, name):
        super().upgrade_state_dict_named(state_dict, name)
        """Upgrade a (possibly old) state dict for new versions."""
        return state_dict

    @classmethod
    def build_model(cls, cfg, task=None):
        """Build a new model instance."""
        return cls(cfg)

    def sample_negatives(self, y, num):

        if self.n_negatives == 0 and self.cross_sample_negatives == 0:
            return y.new(0)
        
        batch_size, time_size, feature_size = y.shape
        y = y.view(-1, feature_size) # B x T x C -> (B x T) x C

        cross_high = time_size * batch_size
        high = time_size
        with torch.no_grad():
            assert high > 1, f"{batch_size, time_size, feature_size}"

            if self.n_negatives > 0:
                time_sizes = (
                    buffered_arange(num)
                    .unsqueeze(-1)
                    .expand(-1, self.n_negatives)
                    .flatten()
                )
                neg_idxs = torch.randint(
                    low = 0, high = high - 1, size = (batch_size, self.n_negatives * num)
                )
                neg_idxs[neg_idxs >= time_sizes] += 1

            if self.cross_sample_negatives > 0:
                time_sizes = (
                    buffered_arange(num)
                    .unsqueeze(-1)
                    .expand(-1, self.cross_sample_negatives)
                    .flatten()
                )

                cross_neg_idxs = torch.randint(
                    low = 0,
                    high = cross_high - 1,
                    size = (batch_size, self.cross_sample_negatives * num)
                )
                cross_neg_idxs[cross_neg_idxs >= time_sizes] += 1
        
        if self.n_negatives > 0:
            for i in range(1, batch_size):
                neg_idxs[i] += i * high
        else:
            neg_idxs = cross_neg_idxs

        if self.cross_sample_negatives > 0 and self.n_negatives > 0:
            neg_idxs = torch.cat([neg_idxs, cross_neg_idxs], dim =1)

        negs = y[neg_idxs.view(-1)]
        negs = negs.view(
            batch_size, num, self.n_negatives + self.cross_sample_negatives, feature_size
        ).permute(
            2, 0, 1, 3
        ) # to N x B x T x C

        return negs, neg_idxs
    
    def compute_preds(self, x, y, negatives):

        neg_is_pos = (y == negatives).all(-1)
        y = y.unsqueeze(0)
        targets = torch.cat([y, negatives], dim= 0)

        logits = torch.cosine_similarity(x.float(), targets.float(), dim = -1).type_as(x)

        logits /= self.logit_temp

        if neg_is_pos.any():
            logits[1:][neg_is_pos] = float("-inf")

        return logits
    
    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):
        """
        Computes the output length of the convolutional layers
        """

        def _conv_out_length(input_length, kernel_size, stride):
            return torch.floor((input_length - kernel_size) / stride + 1)
        
        conv_cfg_list = eval(self.cfg.conv_feature_layers)

        for i in range(len(conv_cfg_list)):
            input_lengths = _conv_out_length(input_lengths, conv_cfg_list[i][1], conv_cfg_list[i][2])
        
        return input_lengths.to(torch.long)

    def forward(
        self,
        source,
        padding_mask = None,
        mask = True,
        features_only = False,
        mask_indices = None,
    ):
        #Masked source
        source_clone = source.clone() #[64, 12, 2500]
        masked_content = torch.zeros_like(source_clone)
        source_clone[:,11,:] = masked_content[:, 11, :]

        if self.feature_grad_mult > 0: #0.1
            features = self.feature_extractor(source) #4 conv blocks of stride=2 #[64, 256, 156]
            masked_features = self.feature_extractor(source_clone)
            if self.feature_grad_mult != 1.0:
                features = GradMultiply.apply(features, self.feature_grad_mult) #multiply features by 0.1
                masked_features = GradMultiply.apply(masked_features, self.feature_grad_mult)  # multiply features by 0.1
        else:
            with torch.no_grad():
                features = self.feature_extractor(source)
                masked_features = self.feature_extractor(source_clone)

        features_pen = features.float().pow(2).mean()

        features = features.transpose(1,2) #[64, 156, 256]
        features = self.layer_norm(features) #features 축을 가장 뒤로 보낸 뒤, 해당 축에 대한 normalization 진행
        unmasked_features = features.clone() #clone the features (for unmasked features)
        features_unmasked = features.clone()

        masked_features =masked_features.transpose(1,2)
        masked_features = self.layer_norm(masked_features)

        if padding_mask is not None and padding_mask.any():
            input_lengths = (1 - padding_mask.long()).sum(-1)
            if input_lengths.dim() > 1:
                for input_len in input_lengths:
                    assert (input_len == input_len[0]).all()
                input_lengths = input_lengths[:,0]
            # apply conv formula to get real output_lengths
            output_lengths = self._get_feat_extract_output_lengths(input_lengths)

            padding_mask = torch.zeros(
                features.shape[:2], dtype = features.dtype, device = features.device
            )
            padding_mask = torch.zeros(
                masked_features.shape[:2], dtype = masked_features.dtype, device = masked_features.device
            )

            # these two operations makes sure that all values
            # before the output lengths indices are attended to
            padding_mask[
                (
                    torch.arange(padding_mask.shape[0], device = padding_mask.device),
                    output_lengths - 1
                )
            ] = 1
            padding_mask = (1 - padding_mask.flip([-1]).cumsum(-1).flip([-1])).bool()
        else:
            padding_mask = None

        if self.post_extract_proj is not None: #feature을 더 높은 차원으로 임베딩 256 -> 768
            features = self.post_extract_proj(features)
            features_unmasked = self.post_extract_proj(features_unmasked)
            masked_features = self.post_extract_proj(masked_features)

        features_unmasked = self.dropout_input(features_unmasked)
        features = self.dropout_input(features) #[64, 156, 768] 0.1프로의 확률로 dropout 적용
        unmasked_features = self.dropout_features(unmasked_features)
        masked_features = self.dropout_input(masked_features)  # [64, 156, 768] 0.1프로의 확률로 dropout 적용

        num_vars = None
        code_ppl = None
        prob_ppl = None
        curr_temp = None

        if self.input_quantizer:
            q = self.input_quantizer(features, produce_targets=False)
            features = q["x"]
            num_vars = q["num_vars"]
            code_ppl = q["code_perplexity"]
            prob_ppl = q["prob_perplexity"]
            curr_temp = q["temp"]
            features = self.project_inp(features)

        if mask:
            x, mask_indices = self.apply_mask(
                features,
                padding_mask,
                mask_indices = mask_indices
            )
            n = masked_features
            if mask_indices is not None:
                y = unmasked_features[mask_indices].view(
                    unmasked_features.size(0), -1, unmasked_features.size(-1) #[64, 59, 256]
                )
            else:
                y = unmasked_features
        else:
            n = masked_features
            x = features
            y = unmasked_features
            features_unmasked = features_unmasked
            mask_indices = None
        
        x = self.encoder(x, padding_mask = padding_mask) #[64, 156, 768]
        features_unmasked = self.encoder(features_unmasked)
        n = self.encoder(n, padding_mask = padding_mask)

        #deconvolution
        n = self.transpose_feature(n)

        if features_only:
            return {"x": x, "padding_mask" : padding_mask, "features": unmasked_features}
        
        if self.quantizer:
            q = self.quantizer(y, produce_targets=False)
            y = q["x"] #[64, 59, 256]
            num_vars = q["num_vars"]
            code_ppl = q["code_perplexity"]
            prob_ppl = q["prob_perplexity"]
            curr_temp = q["temp"]

            y = self.project_q(y) #[64, 59, 256]

            if self.negatives_from_everywhere:
                neg_cands, *_ = self.quantizer(unmasked_features, produce_targets = False)
                negs, _ = self.sample_negatives(neg_cands, y.size(1))
                negs = self.project_q(negs)
            else:
                negs, _ = self.sample_negatives(y, y.size(1))

            if self.codebook_negatives > 0:
                cb_negs = self.quantizer.sample_from_codebook(
                    y.size(0) * y.size(1), self.codebook_negatives
                )
                cb_negs = cb_negs.view(
                    self.codebook_negatives, y.size(0), y.size(1), -1
                ) # order does not matter
                cb_negs = self.project_q(cb_negs)
                negs = torch.cat([negs, cb_negs], dim = 0)
        else:
            y = self.project_q(y)

            if self.negatives_from_everywhere:
                negs, _ = self.sample_negatives(unmasked_features, y.size(1))
                negs = self.project_q(negs)
            else:
                negs, _ = self.sample_negatives(y, y.size(1))

        x = x[mask_indices].view(x.size(0), -1, x.size(-1)) #[64, 59, 768]

        if self.target_glu:
            y = self.target_glu(y)
            negs = self.target_glu(negs)

        x = self.final_proj(x)
        x = self.compute_preds(x, y, negs)

        result = {"x": x, "padding_mask": padding_mask, "features_pen": features_pen, "n": n}

        if prob_ppl is not None:
            result["prob_perplexity"] = prob_ppl
            result["code_perplexity"] = code_ppl
            result["num_vars"] = num_vars
            result["temp"] = curr_temp
            result["n"] = n

        return result
    
    def quantize(self, x):
        assert self.quantizer is not None
        x = self.feature_extractor(x)
        x = x.transpose(1,2)
        x = self.layer_norm(x)
        return self.quantizer.forward_idx(x)

    def extract_features(self, source, padding_mask, mask=False):
        res = self.forward(source, padding_mask, mask = mask, features_only = True)
        return res
    
    def get_logits(self, net_output):
        logits = net_output["x"]
        logits = logits.transpose(0,2)
        logits = logits.reshape(-1, logits.size(-1))
        return logits
    
    def get_targets(self, sample, net_output, expand_steps = True):
        x = net_output["x"]
        return x.new_zeros(x.size(1) * x.size(2), dtype = torch.long)

    def get_extra_losses(self, net_output):
        pen = []

        if "prob_perplexity" in net_output:
            pen.append(
                (net_output["num_vars"] - net_output["prob_perplexity"])
                / net_output["num_vars"]
            )

        if "features_pen" in net_output:
            pen.append(net_output["features_pen"])

        return pen

    def MSE_losses(self, deconv_output_unmasked, deconv_output):
        MSEloss = torch.nn.MSELoss()
        error = MSEloss(deconv_output_unmasked, deconv_output)

        return error

    def remove_pretraining_modules(self):
        self.quantizer = None
        self.project_q = None
        self.target_glu = None
        self.final_proj = None
